# RT Sentiment API

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/release/python-3110/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)

A high-performance, real-time sentiment analysis API built with FastAPI and optimized for **sub-100ms latency**. Features fine-tuned DistilBERT models with both PyTorch and ONNX inference backends.

## ðŸš€ Features

- **âš¡ Ultra-fast inference**: <100ms median latency target
- **ðŸ”„ Async processing**: Non-blocking request handling with background batching
- **ðŸ“¦ Multiple model formats**: PyTorch and ONNX Runtime support
- **ðŸŽ¯ High accuracy**: Fine-tuned DistilBERT on sentiment datasets
- **ðŸ“Š Production ready**: Comprehensive monitoring, logging, and health checks
- **ðŸ³ Containerized**: Docker images optimized for training and inference
- **â˜¸ï¸ Kubernetes ready**: Helm charts and deployment manifests included
- **ðŸ§ª Thoroughly tested**: Unit tests, integration tests, and performance benchmarks

## ðŸ“‹ Quick Start

### Local Development

```bash
# Clone the repository
git clone https://github.com/DanEstok/rt-sentiment-api.git
cd rt-sentiment-api

# Install dependencies
pip install -r requirements.txt

# Train a model (optional - for development)
python src/training/train.py --dataset twitter --epochs 1 --export

# Start the API server
python src/app/main.py
```

### Docker

```bash
# Build and run with Docker Compose
docker-compose up -d

# Test the API
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "I love this product!"}'
```

### API Usage

```python
import requests

# Single prediction
response = requests.post(
    "http://localhost:8000/predict",
    json={"text": "This product is amazing!"}
)
result = response.json()
print(f"Sentiment: {result['label']} (confidence: {result['confidence']:.2f})")

# Batch prediction
response = requests.post(
    "http://localhost:8000/predict/batch",
    json={"texts": ["Great product!", "Poor quality.", "Average experience."]}
)
results = response.json()
for i, result in enumerate(results):
    print(f"Text {i+1}: {result['label']}")
```

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Load Balancer â”‚    â”‚   API Gateway   â”‚    â”‚   Monitoring    â”‚
â”‚    (nginx)      â”‚    â”‚   (FastAPI)     â”‚    â”‚ (Prometheus)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Inference     â”‚    â”‚   Batch Queue   â”‚    â”‚   Model Cache   â”‚
â”‚   Service       â”‚    â”‚   (asyncio)     â”‚    â”‚   (Memory)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ML Models     â”‚
                    â”‚ (PyTorch/ONNX)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“Š Performance

### Latency Targets

- **Primary Target**: <100ms median latency
- **Hard Requirement**: <120ms median latency on CPU
- **P95 Target**: <150ms
- **P99 Target**: <300ms

### Benchmark Results

| Test Type | Requests | Success Rate | RPS | Avg Latency | Median | P95 | P99 |
|-----------|----------|--------------|-----|-------------|--------|-----|-----|
| Single    | 1000     | 99.8%        | 45.2| 52.3ms      | 48.1ms | 89.2ms | 156.7ms |
| Concurrent| 1000     | 99.5%        | 67.8| 67.8ms      | 62.3ms | 125.4ms | 189.2ms |
| Batch-5   | 200      | 100.0%       | 234.5| 21.3ms     | 19.8ms | 34.2ms | 45.6ms |

## ðŸ› ï¸ Development

### Prerequisites

- Python 3.11+
- Docker and Docker Compose
- 8GB+ RAM (for model training)

### Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up pre-commit hooks (optional)
pip install pre-commit
pre-commit install

# Run tests
pytest tests/ -v

# Run linting
ruff check src/ tests/ scripts/
black src/ tests/ scripts/
```

### Training Models

```bash
# Train on Twitter dataset
python src/training/train.py \
  --dataset twitter \
  --epochs 3 \
  --batch_size 16 \
  --learning_rate 2e-5 \
  --export

# Train on Trustpilot dataset
python src/training/train.py \
  --dataset trustpilot \
  --epochs 5 \
  --batch_size 32 \
  --export

# Docker training with GPU
docker run --gpus all \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/data:/app/data \
  sentiment-training \
  python src/training/train.py --dataset twitter --epochs 3 --export
```

## ðŸš€ Deployment

### Docker

```bash
# Build images
docker build -f docker/Dockerfile.training -t sentiment-training .
docker build -f docker/Dockerfile.inference -t sentiment-api .

# Run inference service
docker run -p 8000:8000 \
  -v $(pwd)/models:/app/models:ro \
  sentiment-api
```

### Kubernetes

```bash
# Deploy with kubectl
kubectl apply -f deploy/k8s/

# Deploy with Helm
helm install sentiment-api ./deploy/helm/sentiment-api \
  --namespace sentiment-api \
  --create-namespace \
  --values ./deploy/helm/sentiment-api/values-prod.yaml
```

### Production Checklist

- [ ] Models trained and exported to ONNX
- [ ] Performance benchmarks meet requirements
- [ ] Health checks configured
- [ ] Monitoring and alerting set up
- [ ] Resource limits configured
- [ ] Security policies applied
- [ ] Load testing completed

## ðŸ“– API Reference

### Endpoints

#### `POST /predict`

Analyze sentiment for a single text.

**Request:**
```json
{
  "text": "I love this product!"
}
```

**Response:**
```json
{
  "label": "positive",
  "score": 0.9234,
  "confidence": 0.9234
}
```

#### `POST /predict/batch`

Analyze sentiment for multiple texts.

**Request:**
```json
{
  "texts": ["Great product!", "Poor quality.", "Average experience."]
}
```

**Response:**
```json
[
  {"label": "positive", "score": 0.9234, "confidence": 0.9234},
  {"label": "negative", "score": 0.8756, "confidence": 0.8756},
  {"label": "neutral", "score": 0.7123, "confidence": 0.7123}
]
```

#### `GET /healthz`

Health check endpoint.

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

#### `GET /metrics`

Prometheus metrics endpoint.

#### `GET /model/info`

Get information about the loaded model.

## ðŸ§ª Testing

### Running Tests

```bash
# Run all tests
pytest

# Run specific test categories
pytest -m "not performance"  # Skip performance tests
pytest -m integration        # Only integration tests

# Run with coverage
pytest --cov=src --cov-report=html

# Run performance tests
pytest tests/test_latency.py -v -s
```

### Benchmarking

```bash
# Quick benchmark
python scripts/benchmark.py --requests 100 --concurrency 10

# Comprehensive benchmark
python scripts/benchmark.py \
  --requests 1000 \
  --concurrency 20 \
  --users 15 \
  --duration 120 \
  --test all \
  --output results.json
```

## ðŸ“Š Monitoring

### Metrics

The API exposes Prometheus metrics at `/metrics`:

- `sentiment_requests_total`: Total number of requests
- `sentiment_request_duration_seconds`: Request latency histogram
- `sentiment_prediction_duration_seconds`: Model inference latency

### Health Checks

- **Liveness**: `/healthz` - Basic service health
- **Readiness**: `/healthz` - Model loaded and ready

### Logging

Structured JSON logging with correlation IDs:

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "message": "Prediction completed",
  "request_id": "req-123",
  "latency_ms": 45.2,
  "text_length": 25
}
```

## ðŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MODEL_PATH` | `./models` | Path to model files |
| `LOG_LEVEL` | `INFO` | Logging level |
| `BATCH_SIZE` | `8` | Default batch size |
| `MAX_BATCH_SIZE` | `32` | Maximum batch size |
| `BATCH_TIMEOUT` | `0.01` | Batch timeout in seconds |
| `PORT` | `8000` | Server port |
| `HOST` | `0.0.0.0` | Server host |

### Model Configuration

```python
# Model settings
MODEL_NAME = "distilbert-base-uncased"
NUM_LABELS = 3  # negative, neutral, positive
MAX_LENGTH = 512
```

## ðŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](docs/contributing.md) for details.

### Quick Contribution Steps

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass (`pytest`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

### Development Guidelines

- Follow PEP 8 style guidelines
- Add type hints to all functions
- Write comprehensive tests
- Update documentation for new features
- Ensure performance requirements are met

## ðŸ“š Documentation

- [Architecture Overview](docs/architecture.md)
- [Model Training Guide](docs/model-training.md)
- [API Reference](docs/api-reference.md)
- [Deployment Guide](docs/deployment-guide.md)
- [Benchmark Documentation](docs/benchmark.md)
- [Contributing Guide](docs/contributing.md)

## ðŸ”’ Security

### Reporting Security Issues

Please report security vulnerabilities to support@sparktechrepair.com. Do not open public issues for security concerns.

### Security Features

- Input validation and sanitization
- Rate limiting (configurable)
- Container security best practices
- Non-root container execution
- Network policies for Kubernetes

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- [Hugging Face](https://huggingface.co/) for the Transformers library
- [FastAPI](https://fastapi.tiangolo.com/) for the excellent web framework
- [DistilBERT](https://arxiv.org/abs/1910.01108) authors for the efficient model architecture
- The open-source community for inspiration and tools

## ðŸ“ž Support

- **Documentation**: Check the [docs](docs/) directory
- **Issues**: [GitHub Issues](https://github.com/DanEstok/rt-sentiment-api/issues)
- **Discussions**: [GitHub Discussions](https://github.com/DanEstok/rt-sentiment-api/discussions)
- **Email**: support@sparktechrepair.com

---

**Built with â¤ï¸ for real-time sentiment analysis**
